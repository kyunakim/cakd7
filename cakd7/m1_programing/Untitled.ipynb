{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff58fa4-4c38-4277-bb37-fdadcbd0571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색 키워드를 입력해주세요  AI\n",
      "필요한 뉴스기사의 숫자를 입력해주세요  1000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         page_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m news_df\n\u001b[0;32m---> 47\u001b[0m \u001b[43mNews\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mNews\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m news_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mget_text()\n\u001b[1;32m     38\u001b[0m news_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m news_df\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnews_contents_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# news_df.loc[i,'url'] = text['href']\u001b[39;00m\n\u001b[1;32m     41\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mnews_contents_crawler\u001b[0;34m(news_url)\u001b[0m\n\u001b[1;32m     10\u001b[0m contents\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m news_url:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#각 기사 html get하기\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     news \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(i)\n\u001b[1;32m     14\u001b[0m     news_html \u001b[38;5;241m=\u001b[39m BeautifulSoup(news\u001b[38;5;241m.\u001b[39mtext,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m#기사 내용 가져오기 (p태그의 내용 모두 가져오기) \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib\n",
    "import urllib.request as rq\n",
    "import pandas as pd\n",
    "\n",
    "global news_df\n",
    "\n",
    "\n",
    "def News():\n",
    "    global news_df\n",
    "    \n",
    "    keyword = input('검색 키워드를 입력해주세요 ')\n",
    "    keyword = urllib.parse.quote(keyword) #퍼센트인코딩 #URL에 문자를 표현하는 인코딩방법(URL에 한글이 섞이면 오류 발생)\n",
    "    num = int(input('필요한 뉴스기사의 숫자를 입력해주세요 '))\n",
    "    \n",
    "    news_df = pd.DataFrame(columns=['title','url']) \n",
    "    \n",
    "    page_num=1\n",
    "    i=0 #크롤링한 기사의 수\n",
    "    while num > i:\n",
    "        url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query='+keyword+'&start='+str(page_num)\n",
    "        html = rq.urlopen(url)\n",
    "        bs = BeautifulSoup(html,'html.parser') ##html_doc 데이터를 html.parser로 파싱한 뒤 BeautifulSoup 객체를 생성해서 bs 변수에 넣음 \n",
    "        texts = bs.find_all(class_='news_tit')\n",
    "\n",
    "        for text in texts:\n",
    "            news_df.loc[i,'title'] = text.get_text()\n",
    "            news_df.loc[i,'url'] = text.get('href')\n",
    "            news_df.loc[i,'content'] = text.get('p')\n",
    "            \n",
    "            # news_df.loc[i,'url'] = text['href']\n",
    "            i +=1\n",
    "            if i == num:\n",
    "                break\n",
    "        page_num +=10\n",
    "    return news_df\n",
    "\n",
    "News()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7c0dfe-fedc-4924-ab9b-6722ce38d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스기사 내용 크롤링하는 함수 만들기(각 뉴스의 url)\n",
    "def news_contents_crawler(news_url):\n",
    "    contents=[]\n",
    "    for i in news_url:\n",
    "        #각 기사 html get하기\n",
    "        news = requests.get(i)\n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "            #기사 내용 가져오기 (p태그의 내용 모두 가져오기) \n",
    "        contents.append(news_html.find_all('p'))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5842d94-adad-4098-8a1e-d5c8b5405c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#html생성해서 기사크롤링하는 함수 만들기(제목,url): 3개의 값을 반환함(제목, 링크, 내용)\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(i)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    # 검색결과\n",
    "    articles = html.select(\"div.group_news > ul.list_news > li div.news_area > a\")\n",
    "    title = news_attrs_crawler(articles,'title')\n",
    "    url = news_attrs_crawler(articles,'href')\n",
    "    content = news_contents_crawler(url)\n",
    "    return title, url, content #3개의 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a66ca-ba17-49eb-9aaa-055e859a64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "for i in url:\n",
    "    title, url,content = articles_crawler(url)\n",
    "    news_titles.append(title)\n",
    "    news_url.append(url)\n",
    "    news_contents.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fc654-f579-4de4-8195-4556a7923933",
   "metadata": {},
   "outputs": [],
   "source": [
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "#제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "    \n",
    "#제목, 링크, 내용 담을 리스트 생성\n",
    "news_titles_1, news_url_1, news_contents_1 = [],[],[]\n",
    "\n",
    "#1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_titles_1,news_titles)\n",
    "makeList(news_url_1,news_url)\n",
    "makeList(news_contents_1,news_contents)\n",
    "\n",
    "\n",
    "#데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'title':news_titles_1,'link':news_url_1,'content':news_contents_1})\n",
    "news_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
